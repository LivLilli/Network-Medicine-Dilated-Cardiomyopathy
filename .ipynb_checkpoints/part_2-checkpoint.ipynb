{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import markov_clustering as mc\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import hypergeom\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class task_2_1_a:\n",
    "    def __init__(self):\n",
    "        self.union_df = pd.read_csv('results/task_1_3_union.csv', index_col=0, sep=';')\n",
    "        self.sgi_df = pd.read_csv('results/task_1_3_sgi.csv', index_col=0)\n",
    "        self.intersection_df = pd.read_csv('results/task_1_3_intersection.csv', index_col=0)\n",
    "        \n",
    "        self.r_partial_union = pd.read_csv('data/task_2_1_a_partial_union.csv', index_col=0).T\n",
    "        self.r_partial_sgi = pd.read_csv('data/task_2_1_a_partial_sgi.csv').T.drop('Unnamed: 0')\n",
    "        self.r_partial_intersection = pd.read_csv('data/task_2_1_a_partial_intersection.csv').T.drop('Unnamed: 0')\n",
    "    @staticmethod\n",
    "    def global_measures(df, type_net):\n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "            - df = dataframe of interactions;\n",
    "            \n",
    "            - type_net = string which represents the interactome type (union, intersection or sgi).\n",
    "            \n",
    "        Returns:\n",
    "            IF graph with more than 20 nodes:\n",
    "                - result df = dataframe with partial results for the input graph:\n",
    "                    - if intersection and sgi graphs (more connected components): centralization measure is missing beacuse computed on R; \n",
    "                    \n",
    "                    - if union graph (one connected component): centralization, avg sp, diameter and radius are missing becuase computed on R:\n",
    "                        igraph package on R has better computational performances for high dimensional graphs);\n",
    "\n",
    "            ELSE:\n",
    "                - print string.\n",
    "        '''\n",
    "        result_dict = {}\n",
    "        # undirected graph object\n",
    "        graph = nx.from_pandas_edgelist(df, source = 'interactor A gene symbol', target='interactor B gene symbol')\n",
    "        # check number of nodes\n",
    "        if graph.number_of_nodes() >20:\n",
    "            n_nodes = graph.number_of_nodes()\n",
    "            # not consider the duplicates (union can have same edges with different sources: they will be considered just one time)\n",
    "            n_edges = graph.number_of_edges()\n",
    "            # number of connected components\n",
    "            conn_components = nx.number_connected_components(graph)\n",
    "            # number of isolates\n",
    "            n_isolates = nx.number_of_isolates(graph)\n",
    "            # average degree\n",
    "            #avg_degree = n_edges/n_nodes\n",
    "            degrees = dict(graph.degree())\n",
    "            sum_of_edges = sum(degrees.values())\n",
    "            avg_degree = sum_of_edges/n_nodes\n",
    "            # avg clustering coefficient\n",
    "            avg_cluster_coeff = nx.average_clustering(graph)\n",
    "            # building dictionary of results for th whole graph \n",
    "            result_dict['Nodes'] = [n_nodes]\n",
    "            result_dict['Edges'] = [n_edges]\n",
    "            result_dict['Connected Components'] = [conn_components]\n",
    "            result_dict['Isolates'] = [n_isolates]\n",
    "            result_dict['Avg Degree'] = [avg_degree]\n",
    "            result_dict['Avg Cluster Coeff'] = [avg_cluster_coeff]\n",
    "            \n",
    "\n",
    "            # list of col names\n",
    "            ll=['Graph']\n",
    "            # save adjacency matrix for more computations on R with igraph package (faster)\n",
    "            a= nx.to_numpy_matrix(graph)\n",
    "            try:\n",
    "                os.remove('data/task_2_1_%s'%type_net +'_adj_matrix.csv')\n",
    "            except:\n",
    "                pass\n",
    "            # save to csv\n",
    "            pd.DataFrame(a).to_csv('data/task_2_1_%s'%type_net +'_adj_matrix.csv')\n",
    "\n",
    "            # if graph not connected  (i.e. if more than one connected component)\n",
    "            if conn_components !=1:\n",
    "                result_dict['Avg Shortest Path'] = ['-']\n",
    "                result_dict['Diameter'] = ['-']\n",
    "                result_dict['Radius'] = ['-']\n",
    "                # counter\n",
    "                c = 1\n",
    "                #for each connected component computes the properties\n",
    "                for g in nx.connected_component_subgraphs(graph): \n",
    "                    # update dictionary of results\n",
    "                    result_dict['Nodes'].append(nx.number_of_nodes(g))\n",
    "                    result_dict['Edges'].append(nx.number_of_edges(g))\n",
    "                    result_dict['Avg Shortest Path'].append(nx.average_shortest_path_length(g))\n",
    "                    result_dict['Diameter'].append(nx.diameter(g))\n",
    "                    result_dict['Radius'].append(nx.radius(g))\n",
    "                    result_dict['Avg Degree'].append('-')\n",
    "                    result_dict['Avg Cluster Coeff'].append('-')\n",
    "                    result_dict['Isolates'].append('-')\n",
    "                    result_dict['Connected Components'].append('-')\n",
    "                    # update list of cols names\n",
    "                    ll.append('Connected Component %d'%c)\n",
    "                    # update counter\n",
    "                    c +=1\n",
    "            # from dictionary to df\n",
    "            result_df = pd.DataFrame.from_dict(result_dict, orient = 'index', columns=ll)\n",
    "            # return df\n",
    "            return result_df\n",
    "        # if n nodes <= 20, then stop\n",
    "        else:\n",
    "\n",
    "            print('%s'%type_net,'network do not have a number of edges bigger than 20')\n",
    "            \n",
    "            \n",
    "    @staticmethod\n",
    "    def final_sgi_inters(partial, r_partial):\n",
    "        r_partial.rename(columns={0:'Graph'}, inplace = True)\n",
    "        ll = ['-' for x in range(len(partial.columns[1:]))]\n",
    "        na_df = pd.DataFrame([ll,ll,ll], columns = partial.columns[1:], index=['Btw Centralization', 'Degree Centralization', 'Eigen Centralization'])\n",
    "        r_partial = pd.concat([r_partial,na_df],axis=1)\n",
    "        final_df = pd.concat([partial,r_partial])\n",
    "        return final_df\n",
    "    \n",
    "    def compute_global_measures(self):\n",
    "        partial_union = task_2_1_a.global_measures(self.union_df, 'union')\n",
    "        partial_sgi = task_2_1_a.global_measures(self.sgi_df, 'sgi')\n",
    "        partial_intersection = task_2_1_a.global_measures(self.intersection_df, 'intersection')\n",
    "        \n",
    "        \n",
    "        self.r_partial_union.rename(columns={1:'Graph'}, inplace = True)\n",
    "        final_union_df = pd.concat([partial_union, self.r_partial_union])\n",
    "        \n",
    "        \n",
    "        final_sgi_df = task_2_1_a.final_sgi_inters(partial_sgi, self.r_partial_sgi)\n",
    "        final_intersection_df = task_2_1_a.final_sgi_inters(partial_intersection,self.r_partial_intersection)\n",
    "        return final_union_df,final_sgi_df, final_intersection_df\n",
    "    \n",
    "    def save_results(self):\n",
    "        union_df, sgi_df, intersection_df = self.compute_global_measures()\n",
    "        try:\n",
    "            os.remove('results/task_2_1_union.csv')\n",
    "            os.remove('results/task_2_1_sgi.csv')\n",
    "            os.remove('results/task_2_1_intersection.csv')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        union_df.to_csv('results/task_2_1_union.csv')\n",
    "        sgi_df.to_csv('results/task_2_1_sgi.csv')\n",
    "        intersection_df.to_csv('results/task_2_1_intersection.csv')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 2.1 A\n",
    "taskA = task_2_1_a()\n",
    "taskA.save_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class LCC\n",
    "\n",
    "Save to csv the adjacency matrix in order to use it on R.\n",
    "\n",
    "Compute global and local measures of the largest connected component \n",
    "    of the graph corresponding to the given df.\n",
    "'''\n",
    "\n",
    "class LCC(object):\n",
    "    def __init__(self, dataframe, data_type):\n",
    "        '''\n",
    "        - dataframe: dataset corresponding to the union or the intersection\n",
    "        \n",
    "        - data_type: string to indicate type of df ('union' or 'intersection')\n",
    "        '''\n",
    "        \n",
    "        self.df = dataframe\n",
    "        self.data_type = data_type\n",
    "    \n",
    "    def lcc_graph(self):\n",
    "        '''\n",
    "        Returns:\n",
    "        \n",
    "            - largest connected component graph\n",
    "        '''\n",
    "        # create graph from df\n",
    "        graph = nx.from_pandas_edgelist(self.df, source = 'interactor A gene symbol', target='interactor B gene symbol')\n",
    "        conn_components = nx.number_connected_components(graph)\n",
    "        if conn_components ==1:\n",
    "            return graph\n",
    "        else:\n",
    "            # return set of nodes of the largest connected component\n",
    "            lcc_set = max(nx.connected_components(graph), key=len)\n",
    "            # lcc subgraph\n",
    "            lcc_graph = graph.subgraph(lcc_set)\n",
    "            return lcc_graph\n",
    "\n",
    "    def lcc_a_matrix_to_csv(self):\n",
    "        '''\n",
    "        Returns:\n",
    "        \n",
    "            - file .csv with the adjacency matrix of the graph corresponding \n",
    "                to the df given in input to the class. \n",
    "        \n",
    "        It is necessary saving the matrix on an external file in order to use it for more \n",
    "            computations in R, where specific functions are faster.\n",
    "        '''\n",
    "    \n",
    "        # lcc subgraph\n",
    "        lcc_graph = self.lcc_graph()\n",
    "        # numpy adj matrix\n",
    "        a= nx.to_numpy_matrix(lcc_graph)\n",
    "        try:\n",
    "            os.remove('data/task_2_1_%s'%self.data_type +'_lcc_matrix.csv')\n",
    "        except:\n",
    "            pass\n",
    "        # save to csv\n",
    "        #np.savetxt('data/%s'%data_type+'_lcc_matrix.csv', a, delimiter=\",\", header='')\n",
    "        pd.DataFrame(a).to_csv('data/task_2_1_%s'%self.data_type +'_lcc_matrix.csv')\n",
    "\n",
    "    def task_2_1_b_global(self):\n",
    "        '''\n",
    "        Returns:\n",
    "        \n",
    "            - df: dataframe of global measures for the graph which corresponds to the \n",
    "                input dataframe (union or inters).\n",
    "            \n",
    "            - nodes: list of nodes names of the graph.\n",
    "        '''\n",
    "        # lcc subgraph\n",
    "        lcc_graph = self.lcc_graph()\n",
    "        # number of nodes\n",
    "        n_nodes = lcc_graph.number_of_nodes()\n",
    "        # number of edges\n",
    "        # not consider the duplicates (union can have same edges with different sources: they will be considered just one time)\n",
    "        n_edges = lcc_graph.number_of_edges()\n",
    "        # average degree\n",
    "        degrees = dict(lcc_graph.degree())\n",
    "        sum_of_edges = sum(degrees.values())\n",
    "        avg_degree = sum_of_edges/n_nodes\n",
    "        # avg clustering coefficient\n",
    "        avg_cluster_coeff = nx.average_clustering(lcc_graph)\n",
    "        # create df\n",
    "        df = pd.DataFrame([n_nodes, n_edges, avg_degree, avg_cluster_coeff])\n",
    "        df.rename({0:'Nodes', 1:'Edges', 2:'Avg Degree', 3:'Avg Clustering Coeff'}, inplace = True)\n",
    "        # lcc nodes\n",
    "        nodes = list(lcc_graph.nodes())\n",
    "        return df, nodes\n",
    "\n",
    "    def merge_global_measures(self):\n",
    "        '''\n",
    "        Return:\n",
    "        \n",
    "            - dataframe containing the global measures of the graph computed on Python and R.\n",
    "                In particular there are: number of edges and nodes, avg degree, avg clustering coefficient, \n",
    "                    avg. path length, diameter and radius. \n",
    "        \n",
    "        '''\n",
    "        # compute the above function to obtain global measures\n",
    "        lcc_global1, lcc_nodes  = self.task_2_1_b_global()\n",
    "        # upload the df containg avg shortest path, diameter and radius\n",
    "        # it was computed on R because of the better computational time of the functions\n",
    "        lcc_global2 = pd.read_csv('data/task_2_1_%s'%self.data_type+'_lcc_global_results.csv')\n",
    "        # concatenate the 2 dataframe in one\n",
    "        lcc_global = pd.concat([lcc_global1.T, lcc_global2], axis = 1).drop(['Unnamed: 0'], axis=1)\n",
    "        lcc_global = lcc_global.rename({0:'lcc_%s'%self.data_type}) \n",
    "        \n",
    "        return lcc_global\n",
    "    \n",
    "    def save_local_results(self):\n",
    "        '''\n",
    "        Returns:\n",
    "        \n",
    "            - file .csv with the local measures related to the graph of the input df (union or intersection)\n",
    "        '''\n",
    "        # lcc nodes names\n",
    "        _,lcc_nodes = self.task_2_1_b_global()\n",
    "        # load local dfs computed on R because of the better computational time\n",
    "        lcc_local = pd.read_csv('data/task_2_1_%s'%self.data_type +'_lcc_local_results.csv').drop('Unnamed: 0', axis=1)\n",
    "        # rename rows with gene names\n",
    "        lcc_local = lcc_local.rename(dict(zip([x for x in range(len(lcc_local))],lcc_nodes)))\n",
    "        # save local df results\n",
    "        try:\n",
    "            os.remove('results/task_2_1_%s'%self.data_type +'_lcc_local.csv')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        lcc_local.to_csv('results/task_2_1_%s'%self.data_type +'_lcc_local.csv')\n",
    "        \n",
    "    def mcl_algo(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            \n",
    "            - Scipy sparse adjacency matrix.\n",
    "            - List of clusters: each cluster is a sublist containing the nodes which belongs to it.\n",
    "            \n",
    "        '''\n",
    "        graph = self.lcc_graph()\n",
    "        adj_matrix = nx.to_scipy_sparse_matrix(graph)\n",
    "        ### !!! attention: it works only with scipy version 1.2.0 !!!\n",
    "        result = mc.run_mcl(adj_matrix)           # run MCL with default parameters\n",
    "        clusters = mc.get_clusters(result)  # get clusters\n",
    "        return adj_matrix,clusters\n",
    "    \n",
    "    \n",
    "    def mcl_plot():\n",
    "        adj_matrix, clusters = self.mcl_algo()\n",
    "        mc.draw_graph(adj_matrix, clusters, node_size=50, with_labels=False, edge_color=\"silver\")\n",
    "        \n",
    "        \n",
    "    def hypergeometric_test(self):\n",
    "        '''\n",
    "        Returns:\n",
    "        \n",
    "            - Dictionary where for each cluster id gives the corresponding p-value.\n",
    "        '''\n",
    "        # list of sublists: each i-sublist contains nodes belogning to i-cluster\n",
    "        _ , lcc_clusters = self.mcl_algo()\n",
    "        # list of lcc nodes names\n",
    "        _, lcc_nodes = self.task_2_1_b_global()\n",
    "        # list of seeds genes\n",
    "        seed_genes = list(pd.read_csv('data/task_1_4_seed_genes.txt', header=None)[0])\n",
    "        # seeds in lcc\n",
    "        seeds_lcc = [x for x in seed_genes if x in lcc_nodes]\n",
    "        # population M = number of genes in lcc\n",
    "        pop = len(lcc_nodes)\n",
    "        #initialize p-values dictionary\n",
    "        p_values = {}\n",
    "        # cluster index\n",
    "        c = 0\n",
    "        for cluster in lcc_clusters:\n",
    "            # n nodes in cluster\n",
    "            cluster_dimension = len(cluster)\n",
    "            # check number of nodes\n",
    "            if cluster_dimension >= 10:\n",
    "                # list of seed genes in cluster\n",
    "                seeds_in_cluster = [lcc_nodes[x] for x in cluster if lcc_nodes[x] in seeds_lcc]\n",
    "                # number of seeds in cluster\n",
    "                # genes in cluster\n",
    "                genes_in_cluster = [lcc_nodes[x] for x in cluster]\n",
    "                # number of seeds in cluster\n",
    "                n_seeds_in_cluster = len(seeds_in_cluster)\n",
    "                # parameter for hypergeom test\n",
    "                M,n,N,x =  pop, cluster_dimension,len(seeds_lcc),n_seeds_in_cluster\n",
    "                pval = hypergeom.sf(x-1, M, n, N)\n",
    "                \n",
    "                p_values[c] = [pval]\n",
    "                p_values[c].append('MCL')\n",
    "                p_values[c].append(n_seeds_in_cluster)\n",
    "                p_values[c].append(cluster_dimension)\n",
    "                p_values[c].append(','.join(seeds_in_cluster))\n",
    "                p_values[c].append(','.join(genes_in_cluster))\n",
    "                \n",
    "            c+=1\n",
    "        # putatitive diseases modules dictionary\n",
    "        pdm_dic = {}\n",
    "        for i in p_values.keys():\n",
    "\n",
    "            if p_values[i][0] < 0.05:\n",
    "                pdm_dic[i] = p_values[i]\n",
    "\n",
    "        return pdm_dic\n",
    "    \n",
    "    def save_test_results(self):\n",
    "\n",
    "        pdm_lcc = self.hypergeometric_test()\n",
    "        lcc_clust_results = pd.DataFrame.from_dict(pdm_lcc).T\n",
    "        new_col_names = ['p-value','Clustering Algorithm', 'Seed Genes Number', 'Total Genes Number', 'Seed Genes ID', 'Genes ID']\n",
    "        lcc_clust_results.rename(columns = dict(zip([x for x in range(len(new_col_names))], new_col_names)), inplace=True)\n",
    "        lcc_clust_results.rename_axis('Cluster ID',inplace=True)\n",
    "        try: \n",
    "            os.remove('results/task_2_2_lcc_%s'%self.data_type +'_putative_disease_modules.csv')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        lcc_clust_results.to_csv('results/task_2_2_lcc_%s'%self.data_type +'_putative_disease_modules.csv')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df = pd.read_csv('results/task_1_3_union.csv', index_col=0, sep=';')\n",
    "intersection_df = pd.read_csv('results/task_1_3_intersection.csv', index_col=0)\n",
    "# call the class for the 2 dataframes\n",
    "LCC_union = LCC(union_df, 'union')\n",
    "LCC_inters = LCC(intersection_df, 'intersection')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adjacency matrix for the 2 graphs in order to use it on R\n",
    "LCC_union.lcc_a_matrix_to_csv()\n",
    "LCC_inters.lcc_a_matrix_to_csv()\n",
    "# run r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 2.1 B - GLOBAL\n",
    "# union global measures df \n",
    "lcc_union_global = LCC_union.merge_global_measures()\n",
    "# intersection global measures df \n",
    "lcc_inters_global = LCC_inters.merge_global_measures()\n",
    "# concatenate the 2 final dataframes\n",
    "lcc_global = pd.concat([lcc_union_global, lcc_inters_global], axis = 0)\n",
    "# save resulting df on csv\n",
    "# remove if already existing adn then create the new one\n",
    "try:\n",
    "    os.remove('results/task_2_1_lcc_global.csv')\n",
    "except: \n",
    "    pass\n",
    "lcc_global.to_csv('results/task_2_1_lcc_global.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 2.1 B - LOCAL\n",
    "\n",
    "# save lcc union local results\n",
    "LCC_union.save_local_results()\n",
    "# save lcc intersection local results\n",
    "LCC_inters.save_local_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 2.2\n",
    "pdm_lcc_union = LCC_union.save_test_results()\n",
    "pdm_lcc_inters = LCC_inters.save_test_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
